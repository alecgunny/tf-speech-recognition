{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The New TensorFlow Ecosystem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## (Or how I learned to stop worrying and love the bomb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why TensorFlow?\n",
    "- Community: plenty of code out there to... borrow\n",
    "- When done properly, it's fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Why Keras?\n",
    "- Fast extensibility and customizability for TensorFlow, especially for recurrent models\n",
    "- Addresses a lot of the reasons *not* to use TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why not Horovod?\n",
    "- Distribution API great for single node\n",
    "- Reduces dependencies, one less thing to keep up-to-date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The way things was\n",
    "- TensorFlow pipelines required a lot of recycling custom scaffolding. Lack of APIs or clear organization\n",
    "- Keras helped, but required completely separate scaffolding. Didn't play great with standalone TF\n",
    "- Distribution for both was really bad without Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The way things is\n",
    "- Estimator, Dataset, and Distribution APIs clearly defined, handle things efficiently under the hood\n",
    "- Plays great with Keras. Allows for fast prototyping of new models structures\n",
    "- Effortless near-linear distribution on single node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Long way to go\n",
    "- Still lacking lots of features like learning rate scheduling and Keras model subclassing\n",
    "- TensorFlow documentation can be confusing/lacking/non-standard in its language\n",
    "- Still, represents a popular and reasonably simple way to build and train complicated models quickly\n",
    "- Worth understanding the various pieces and how they play together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow Speech Recognition Challenge\n",
    "Classify the single word spoken in 1 second audio clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/tf-src.png\" width=\"70%\" style=\"position:relative;top:-100px;left:200px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TensorFlow Speech Recognition Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "tf.logging.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# good housekeeping for when we rerun the kernel later\n",
    "! if ! [ -z \"$(ls $MODEL_DIR)\" ]; then rm -r $MODEL_DIR/*; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TensorFlow Speech Recognition Challenge\n",
    "Let's define some (hyper)parameters up front since we'll use them throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Data info\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "TRAIN_DATA = \"{}/train.tfrecords\".format(DATA_DIR)\n",
    "VALID_DATA = \"{}/valid.tfrecords\".format(DATA_DIR)\n",
    "PIXEL_WISE_STATS = \"{}/stats.tfrecords\".format(DATA_DIR)\n",
    "LABELS = \"{}/labels.txt\".format(DATA_DIR)\n",
    "\n",
    "# Model hyperparameters\n",
    "INPUT_SHAPE = (99, 161)\n",
    "NUM_FRAMES = 7\n",
    "FRAME_STEP = 2\n",
    "HIDDEN_SIZES = [1024, 2048, 2048, 1024, 2048]\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_GPUS = 1\n",
    "LEARNING_RATE = 2e-5 * NUM_GPUS\n",
    "DROPOUT = 0.05\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "NUM_TRAIN_SAMPLES = len([record for record in tf.python_io.tf_record_iterator(TRAIN_DATA)])\n",
    "MAX_STEPS = NUM_EPOCHS*NUM_TRAIN_SAMPLES // (BATCH_SIZE*NUM_GPUS)\n",
    "EVAL_THROTTLE_SECS = 120\n",
    "LOG_STEPS = 10\n",
    "MODEL_DIR = os.environ[\"MODEL_DIR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TensorFlow Speech Recognition Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iterator = tf.python_io.tf_record_iterator(TRAIN_DATA)\n",
    "record = next(iterator)\n",
    "features = {\n",
    "    \"spec\": tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True),\n",
    "    \"label\": tf.FixedLenFeature((), tf.string, default_value=\"\")}\n",
    "parsed = tf.parse_single_example(record, features)\n",
    "spectrogram = tf.reshape(parsed[\"spec\"], INPUT_SHAPE)\n",
    "filename = parsed[\"label\"]\n",
    "# filename = \"/data/train/audio/{}\".format(parsed[\"label\"])\n",
    "\n",
    "sess = tf.Session()\n",
    "spec, fname = sess.run([spectrogram, filename])\n",
    "sample_rate, waveform = wavfile.read(fname)\n",
    "word = str(fname).split(\"/\")[-2].title()\n",
    "\n",
    "print(\"Word: {}\".format(word))\n",
    "ipd.Audio(data=waveform, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(waveform)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.set_title(\"\\\"{}\\\": Waveform\".format(word))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(spec[:,::-1])\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_title(\"\\\"{}\\\": Log Spectrogram\".format(word))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spectrogram looks like an image, use a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Lots of good reasons to do this (local information, accounting for \"high\" and \"low\" voices, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=https://media.giphy.com/media/4KMduk6fuPJTi/giphy.gif style=\"position:relative;top:-80px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Baidu <a href=https://arxiv.org/abs/1412.5567>Deep Speech</a>\n",
    "- Recurrent model for end to end speech recognition\n",
    "- Meant for sequence to sequence with different loss function, but forces us to get more creative with `tf.keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=img/deepspeech.PNG width=\"75%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Need to build custom layers\n",
    "- First rolls consecutive spectrogram frames into single timestep\n",
    "- Second does custom forward and backward recurrence with shared parameters\n",
    "- This is where Keras shines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=img/deepspeech.PNG width=\"75%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Custom Layers - Concatenate consecutive frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class ImageToDeepSpeech(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_frames, frame_step, **kwargs):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_step = frame_step\n",
    "        super(ImageToDeepSpeech, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.squeeze(inputs, axis=3)\n",
    "        time_slice = lambda x, i: x[:, i:(-(self.num_frames-1)+i) or None:self.frame_step]\n",
    "        time_shifted_inputs = [time_slice(inputs, i) for i in range(self.num_frames)]\n",
    "        return tf.concat(time_shifted_inputs, axis=2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        time_dim = tf.ceil((shape[1] - self.num_frames + 2) / self.frame_step)\n",
    "        feature_dim = self.num_frames*shape[2]\n",
    "        if self.frame_step == 1:\n",
    "            time_dim += 1\n",
    "        return tf.TensorShape([shape[0], time_dim, feature_dim])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(ImageToDeepSpeech, self).get_config()\n",
    "        base_config['num_frames'] = self.num_frames\n",
    "        base_config['frame_step'] = self.frame_step\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Custom Layer - Custom Recurrent Cell\n",
    "Define the recurrent step like a regular layer. RNN wrapper takes care of rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DeepSpeechCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_size, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        super(DeepSpeechCell, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.matmul(inputs, self.kernel)\n",
    "        u = tf.matmul(prev_output, self.recurrent_kernel)\n",
    "        output = tf.nn.relu(h + u + self.bias)\n",
    "        return output, [output]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[1]\n",
    "        if self.built:\n",
    "            # normally you just return. This is a hack to allow the\n",
    "            # second calling of this cell to use a different recurrent\n",
    "            # kernel. Not elegant but that's showbiz baby\n",
    "            self.recurrent_kernel = self.backward_recurrent_kernel\n",
    "            return\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(self.input_dim, self.state_size),\n",
    "            name='kernel',\n",
    "            initializer='glorot_normal')\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.state_size,),\n",
    "            name='bias',\n",
    "            initializer='zeros')\n",
    "\n",
    "        self.forward_recurrent_kernel = self.add_weight(\n",
    "            shape=(self.state_size, self.state_size),\n",
    "            name='forward_recurrent_kernel',\n",
    "            initializer='glorot_normal')\n",
    "        self.backward_recurrent_kernel = self.add_weight(\n",
    "            shape=(self.state_size, self.state_size),\n",
    "            name='backward_recurrent_kernel',\n",
    "            initializer='glorot_normal')\n",
    "\n",
    "        self.recurrent_kernel = self.forward_recurrent_kernel\n",
    "        super(DeepSpeechCell, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(DeepSpeechCell, self).get_config()\n",
    "        base_config['state_size'] = self.state_size\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def deepspeech_model(\n",
    "        input_shape,\n",
    "        num_frames,\n",
    "        frame_step,\n",
    "        hidden_dims,\n",
    "        num_classes,\n",
    "        dropout=0.05):\n",
    "    # input and convert from image to time series representation\n",
    "    input = tf.keras.Input(shape=input_shape, name='spec')\n",
    "    x = ImageToDeepSpeech(num_frames, frame_step)(input)\n",
    "\n",
    "    # transform with 3 time distributed dense layers\n",
    "    for n, hdim in enumerate(hidden_dims[:3]):\n",
    "        x = tf.keras.layers.Dropout(dropout, name='dropout_{}'.format(n))(x)\n",
    "        dense = tf.keras.layers.Dense(hdim, activation='relu')\n",
    "        x = tf.keras.layers.TimeDistributed(dense, name='dense_{}'.format(n))(x)\n",
    "\n",
    "    # perform forwards and backwards recurrent layers then combine\n",
    "    # note that we're not returning sequences, so we're going from shape\n",
    "    # B x T x F --> B x F\n",
    "    cell = DeepSpeechCell(hidden_dims[3])\n",
    "    forward = tf.keras.layers.RNN(cell, return_sequences=False, name='forward_rnn')(x)\n",
    "    backward = tf.keras.layers.RNN(cell, return_sequences=False, go_backwards=True, name='backward_rnn')(x)\n",
    "    x = tf.keras.layers.Add(name='rnn_combiner')([forward, backward])\n",
    "\n",
    "    # transform with more dense layers (now not time distributed)\n",
    "    for n, hdim in enumerate(hidden_dims[4:]):\n",
    "        x = tf.keras.layers.Dropout(dropout, name='dropout_{}'.format(n+3))(x)\n",
    "        x = tf.keras.layers.Dense(hdim, activation='relu', name='dense_{}'.format(n+3))(x)\n",
    "\n",
    "    # produce output\n",
    "    x = tf.keras.layers.Dropout(dropout, name='dropout_labels')(x)\n",
    "    x = tf.keras.layers.Dense(num_classes, activation='softmax', name='labels')(x)\n",
    "    return tf.keras.Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS, 'r') as f:\n",
    "    labels = f.read().split(\",\")\n",
    "    labels = labels[:20]\n",
    "\n",
    "model = deepspeech_model(INPUT_SHAPE+(1,), NUM_FRAMES, FRAME_STEP, HIDDEN_SIZES, len(labels)+1, DROPOUT)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print('Training for {} steps'.format(MAX_STEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 2. A Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "num_cpus = mp.cpu_count()\n",
    "\n",
    "def input_fn(\n",
    "        dataset_path,\n",
    "        labels,\n",
    "        batch_size,\n",
    "        num_epochs,\n",
    "        input_shape,\n",
    "        stats=None,\n",
    "        eps=0.0001,\n",
    "        buffer_size=50000,\n",
    "        go_fast=False):\n",
    "    dataset = tf.data.TFRecordDataset([dataset_path])\n",
    "    table = tf.contrib.lookup.index_table_from_tensor(\n",
    "        mapping=tf.constant(labels),\n",
    "        num_oov_buckets=1)\n",
    "\n",
    "    # load in pixel wise mean and variance\n",
    "    if stats is not None:\n",
    "        iterator = tf.python_io.tf_record_iterator(stats)\n",
    "        features = {\n",
    "            'mean': tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True),\n",
    "            'var': tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True)}\n",
    "        parsed = tf.parse_single_example(next(iterator), features)\n",
    "\n",
    "        mean = tf.reshape(parsed['mean'], input_shape)\n",
    "        std = tf.reshape(parsed['var']**0.5, input_shape) # square rooting the variance\n",
    "\n",
    "    def parse_spectrogram(record):\n",
    "        features = {\n",
    "            'spec': tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True),\n",
    "            'label': tf.FixedLenFeature((), tf.string, default_value=\"\")}\n",
    "        parsed = tf.parse_single_example(record, features)\n",
    "\n",
    "        # preprocess and normalize spectrogrm\n",
    "        spec = tf.reshape(parsed['spec'], input_shape) # Time steps x Frequency bins\n",
    "        if stats is not None:\n",
    "            spec = (spec - mean) / (std + eps)\n",
    "        spec = tf.expand_dims(spec, axis=2) # add channel dimension, T x F x 1\n",
    "\n",
    "        label = tf.string_split([parsed['label']], delimiter=\"/\").values[-2:-1]\n",
    "        label = table.lookup(label)[0]\n",
    "        label = tf.one_hot(label, len(labels)+1)\n",
    "        return (spec, label)\n",
    "\n",
    "    # naive approach\n",
    "    if not go_fast:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.map(parse_spectrogram, num_parallel_calls=num_cpus)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "    else:\n",
    "        dataset = dataset.apply(\n",
    "            tf.data.experimental.shuffle_and_repeat(buffer_size, num_epochs))\n",
    "        dataset = dataset.apply(\n",
    "            tf.data.experimental.map_and_batch(\n",
    "                map_func=parse_spectrogram,\n",
    "                batch_size=batch_size,\n",
    "                num_parallel_calls=num_cpus))\n",
    "        dataset.prefetch(buffer_size=None) # tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 3. A Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!\n",
    "## 3. A Trainer - Estimator API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimator Training\n",
    "Convert our tf.keras model to a TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\n",
    "    'DeepSpeechCell': DeepSpeechCell,\n",
    "    'ImageToDeepSpeech': ImageToDeepSpeech}\n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "    save_summary_steps=LOG_STEPS,\n",
    "    save_checkpoints_secs=EVAL_THROTTLE_SECS,\n",
    "    log_step_count_steps=LOG_STEPS,\n",
    "    model_dir=MODEL_DIR,\n",
    "#     train_distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS, prefetch_on_device=True)\n",
    ")\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    model,\n",
    "    custom_objects=custom_objects,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimator Training\n",
    "Build a custom hook to monitor total throughput in samples/sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThroughputHook(tf.train.StepCounterHook):\n",
    "    def __init__(self, batch_size, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        super(ThroughputHook, self).__init__(**kwargs)\n",
    "\n",
    "    def begin(self):\n",
    "        super(ThroughputHook, self).begin()\n",
    "        self._summary_tag = 'throughput'\n",
    "\n",
    "    def _log_and_record(self, elapsed_steps, elapsed_time, global_step):\n",
    "        super(ThroughputHook, self)._log_and_record(\n",
    "            elapsed_steps*self.batch_size,\n",
    "            elapsed_time,\n",
    "            global_step)\n",
    "\n",
    "hooks = [ThroughputHook(BATCH_SIZE*NUM_GPUS, every_n_steps=LOG_STEPS, output_dir=estimator.model_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimator Training\n",
    "Build our input functions and train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_FAST = True # set to True in case of emergency\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=lambda : input_fn(\n",
    "        TRAIN_DATA,\n",
    "        labels,\n",
    "        BATCH_SIZE,\n",
    "        NUM_EPOCHS,\n",
    "        INPUT_SHAPE,\n",
    "        stats=PIXEL_WISE_STATS,\n",
    "        go_fast=GO_FAST),\n",
    "    max_steps=MAX_STEPS,\n",
    "    hooks=hooks)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=lambda : input_fn(\n",
    "        VALID_DATA,\n",
    "        labels,\n",
    "        BATCH_SIZE*8,\n",
    "        1,\n",
    "        INPUT_SHAPE,\n",
    "        stats=PIXEL_WISE_STATS,\n",
    "        go_fast=GO_FAST),\n",
    "    throttle_secs=EVAL_THROTTLE_SECS)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!\n",
    "## 3. A Trainer - Estimator API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 4. Multi-GPU trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!\n",
    "## 3. A Trainer - Estimator API!\n",
    "## 4. Multi-GPU trainer -  Distribution API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU\n",
    "Go back to slides above and uncomment the \"Break in case of emergency\" lines, and comment out their existing analogs. Change NUM_GPUs to 4, then click \"Kernel->Restart & Run All\" in the menu bar up top. How does throughput change as we multiply the number of GPUs by 4?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "rise": {
   "autolaunch": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

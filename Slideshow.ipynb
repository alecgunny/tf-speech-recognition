{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The New Kaggle Ecosystem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## (Or how I learned to stop worrying and love the bomb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToDeepSpeech(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_frames, frame_step, **kwargs):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_step = frame_step\n",
    "        super(ImageToDeepSpeech, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.squeeze(inputs, axis=3)\n",
    "        time_slice = lambda x, i: x[:, i:(-(self.num_frames-1)+i) or None:self.frame_step]\n",
    "        time_shifted_inputs = [time_slice(inputs, i) for i in range(self.num_frames)]\n",
    "        return tf.concat(time_shifted_inputs, axis=2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        time_dim = tf.ceil((shape[1] - self.num_frames + 2) / self.frame_step)\n",
    "        feature_dim = self.num_frames*shape[2]\n",
    "        if self.frame_step == 1:\n",
    "            time_dim += 1\n",
    "        return tf.TensorShape([shape[0], time_dim, feature_dim])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(ImageToDeepSpeech, self).get_config()\n",
    "        base_config['num_frames'] = self.num_frames\n",
    "        base_config['frame_step'] = self.frame_step\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DeepSpeechCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_size, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        super(DeepSpeechCell, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.matmul(inputs, self.kernel)\n",
    "        u = tf.matmul(prev_output, self.recurrent_kernel)\n",
    "        output = tf.nn.relu(h + u + self.bias)\n",
    "        return output, [output]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[1]\n",
    "        if self.built:\n",
    "            # normally you just return. This is a hack to allow the\n",
    "            # second calling of this cell to use a different recurrent\n",
    "            # kernel. Not elegant but that's showbiz baby\n",
    "            self.recurrent_kernel = self.backward_recurrent_kernel\n",
    "            return\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(self.input_dim, self.state_size),\n",
    "            name='kernel',\n",
    "            initializer='glorot_normal')\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.state_size,),\n",
    "            name='bias',\n",
    "            initializer='zeros')\n",
    "\n",
    "        self.forward_recurrent_kernel = self.add_weight(\n",
    "            shape=(self.state_size, self.state_size),\n",
    "            name='forward_recurrent_kernel',\n",
    "            initializer='glorot_normal')\n",
    "        self.backward_recurrent_kernel = self.add_weight(\n",
    "            shape=(self.state_size, self.state_size),\n",
    "            name='backward_recurrent_kernel',\n",
    "            initializer='glorot_normal')\n",
    "\n",
    "        self.recurrent_kernel = self.forward_recurrent_kernel\n",
    "        super(DeepSpeechCell, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(DeepSpeechCell, self).get_config()\n",
    "        base_config['state_size'] = self.state_size\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def deepspeech_model(\n",
    "        num_frames,\n",
    "        frame_step,\n",
    "        hidden_dims,\n",
    "        num_classes,\n",
    "        dropout=0.05):\n",
    "    # input and convert from image to time series representation\n",
    "    input = tf.keras.Input(shape=(99, 161, 1), name='spec')\n",
    "    x = ImageToDeepSpeech(num_frames, frame_step)(input)\n",
    "\n",
    "    # transform with 3 time distributed dense layers\n",
    "    for n, hdim in enumerate(hidden_dims[:3]):\n",
    "        x = tf.keras.layers.Dropout(dropout, name='dropout_{}'.format(n))(x)\n",
    "        dense = tf.keras.layers.Dense(hdim, activation='relu')\n",
    "        x = tf.keras.layers.TimeDistributed(dense, name='dense_{}'.format(n))(x)\n",
    "\n",
    "    # perform forwards and backwards recurrent layers then combine\n",
    "    # note that we're not return sequences, so we're going from shape\n",
    "    # B x T x F --> B x F\n",
    "    cell = DeepSpeechCell(hidden_dims[3])\n",
    "    forward = tf.keras.layers.RNN(cell, return_sequences=False, name='forward_rnn')(x)\n",
    "    backward = tf.keras.layers.RNN(cell, return_sequences=False, go_backwards=True, name='backward_rnn')(x)\n",
    "    x = tf.keras.layers.Add(name='rnn_combiner')([forward, backward])\n",
    "\n",
    "    # transform with more dense layers (now not time distributed)\n",
    "    for n, hdim in enumerate(hidden_dims[4:]):\n",
    "        x = tf.keras.layers.Dropout(dropout, name='dropout_{}'.format(n+3))(x)\n",
    "        x = tf.keras.layers.Dense(hdim, activation='relu', name='dense_{}'.format(n+3))(x)\n",
    "\n",
    "    # produce output\n",
    "    x = tf.keras.layers.Dropout(dropout, name='dropout_labels')(x)\n",
    "    x = tf.keras.layers.Dense(num_classes, activation='softmax', name='labels')(x)\n",
    "    return tf.keras.Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 2. A Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "num_cpus = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_input_fn(\n",
    "        dataset_path,\n",
    "        labels,\n",
    "        batch_size,\n",
    "        num_epochs,\n",
    "        mean='/data/mean.npy',\n",
    "        std='/data/std.npy',\n",
    "        eps=0.0001):\n",
    "    def input_fn():\n",
    "        dataset = tf.data.TFRecordDataset([dataset_path])\n",
    "        mean_spec = np.load(mean)\n",
    "        std_spec = np.load(std)**0.5 # saved as variance\n",
    "        table = tf.contrib.lookup.index_table_from_tensor(\n",
    "            mapping=tf.constant(labels),\n",
    "            num_oov_buckets=1)\n",
    "\n",
    "        def parse_spectrogram(record):\n",
    "            features = {\n",
    "                'spec': tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True),\n",
    "                'label': tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
    "            }\n",
    "            parsed = tf.parse_single_example(record, features)\n",
    "\n",
    "            spec = tf.reshape(parsed['spec'], [99, 161]) # Time steps x Frequency bins\n",
    "            spec = (spec - mean_spec) / (std_spec + eps) # normalize\n",
    "            spec = tf.expand_dims(spec, axis=2) # add channel dimension, T x F x 1\n",
    "\n",
    "            label = tf.string_split([parsed['label']], delimiter=\"/\").values[-2:-1]\n",
    "            label = table.lookup(label)[0]\n",
    "            label = tf.one_hot(label, len(labels))\n",
    "            return (spec, label)\n",
    "\n",
    "        # naive approach\n",
    "#         dataset = dataset.shuffle(buffer_size=10000)\n",
    "#         dataset = dataset.repeat(num_epochs)\n",
    "#         dataset = dataset.map(parse_spectrogram)\n",
    "#         dataset = dataset.batch(batch_size)\n",
    "\n",
    "        # BREAK IN CASE OF EMERGENCY\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.shuffle_and_repeat(10000, num_epochs))\n",
    "        dataset = dataset.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                map_func=parse_spectrogram,\n",
    "                batch_size=batch_size,\n",
    "                num_parallel_calls=num_cpus))\n",
    "        dataset.prefetch(buffer_size=None)\n",
    "        return dataset\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What We Need\n",
    "## 1. A Model\n",
    "## 2. A Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 3. A Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Training Script\n",
    "Define some hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Data info\n",
    "TRAIN_DATA = '/data/train.tfrecords'\n",
    "VALID_DATA = '/data/valid.tfrecords'\n",
    "LABELS = '/data/labels.txt'\n",
    "NUM_TRAIN_SAMPLES = 51088\n",
    "\n",
    "# Model hyperparameters\n",
    "NUM_FRAMES = 7\n",
    "FRAME_STEP = 2\n",
    "HIDDEN_SIZES = [1024, 2048, 2048, 1024, 2048]\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 8e-5\n",
    "NUM_EPOCHS = 100\n",
    "NUM_GPUS = 4\n",
    "EVAL_THROTTLE_SECS = 30\n",
    "DROPOUT = 0.05\n",
    "\n",
    "# Quick equivalencies\n",
    "STEPS_PER_EPOCH = NUM_TRAIN_SAMPLES // (BATCH_SIZE * NUM_GPUS)\n",
    "MAX_STEPS = NUM_EPOCHS * STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Training Script\n",
    "Build and compile a tf.keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS, 'r') as f:\n",
    "    labels = f.read().split(\",\")\n",
    "    labels = labels[:20] + ['unknown']\n",
    "\n",
    "model = deepspeech_model(NUM_FRAMES, FRAME_STEP, HIDDEN_SIZES, len(labels), DROPOUT)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Training Script\n",
    "Convert our tf.keras model to a distributed TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAK IN CASE OF EMERGENCY\n",
    "strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS, prefetch_on_device=True)\n",
    "strategy = tf.contrib.distribute.DistributeConfig(train_distribute=strategy)\n",
    "config = tf.estimator.RunConfig(\n",
    "    save_checkpoints_steps=STEPS_PER_EPOCH,\n",
    "    experimental_distribute=strategy)\n",
    "\n",
    "# config w/o distribution\n",
    "# config = tf.estimator.RunConfig(save_checkpoints_steps=STEPS_PER_EPOCH)\n",
    "\n",
    "custom_objects = {\n",
    "    'DeepSpeechCell': DeepSpeechCell,\n",
    "    'ImageToDeepSpeech': ImageToDeepSpeech}\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    model,\n",
    "    custom_objects=custom_objects,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Training Script\n",
    "Get our data generation functions and build training and evaluation specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(\n",
    "    TRAIN_DATA,\n",
    "    labels,\n",
    "    BATCH_SIZE,\n",
    "    NUM_EPOCHS)\n",
    "\n",
    "eval_input_fn = get_input_fn(\n",
    "    VALID_DATA,\n",
    "    labels,\n",
    "    BATCH_SIZE*8,\n",
    "    1)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=MAX_STEPS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, throttle_secs=EVAL_THROTTLE_SECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Training Script\n",
    "Add a metric to keep track of validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    labels = tf.argmax(labels, axis=1)\n",
    "    predictions = tf.argmax(predictions['labels'], axis=1)\n",
    "    return {'accuracy': tf.metrics.accuracy(labels, predictions)}\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Training Script\n",
    "The moment we've all ben waiting for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "rise": {
   "autolaunch": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

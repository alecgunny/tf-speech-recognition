{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The New Kaggle Ecosystem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## (Or how I learned to stop worrying and love the bomb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why TensorFlow?\n",
    "- Community: plenty of code out there to... borrow\n",
    "- When done properly, it's fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Why Keras?\n",
    "- Fast extensibility and customizability for TensorFlow, especially for recurrent models\n",
    "- Addresses a lot of the reasons *not* to use TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why not Horovod?\n",
    "- Distribution API great for single node\n",
    "- Reduces dependencies, one less thing to keep up-to-date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The way things was\n",
    "- TensorFlow pipelines required a lot of recycling custom scaffolding. Lack of APIs or clear organization\n",
    "- Keras helped, but required completely separate scaffolding. Didn't play great with standalone TF\n",
    "- Distribution for both was really bad without Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The way things is\n",
    "- Estimator, Dataset, and Distribution APIs clearly defined, handle things efficiently under the hood\n",
    "- Plays great with Keras. Allows for fast prototyping of new models structures\n",
    "- Effortless near-linear distribution on single node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Long way to go\n",
    "- Still lacking lots of features like learning rate scheduling and Keras model subclassing\n",
    "- TensorFlow documentation can be confusing/lacking/non-standard in its language\n",
    "- Still, represents a popular and reasonably simple way to build and train complicated models quickly\n",
    "- Worth understanding the various pieces and how they play together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow Speech Recognition Challenge\n",
    "Classify the single word spoken in 1 second audio clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"tf-src.png\" width=\"70%\" style=\"position:relative;top:-37px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TensorFlow Speech Recognition Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(0)\n",
    "\n",
    "def parser(record):\n",
    "    features = {\n",
    "        'spec': tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True),\n",
    "        'label': tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
    "    }\n",
    "    parsed = tf.parse_single_example(record, features)\n",
    "    spec = tf.reshape(parsed['spec'], [99, 161])\n",
    "    return spec, parsed['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TensorFlow Speech Recognition Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset('/data/train.tfrecords')\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.map(parser)\n",
    "dataset = dataset.batch(1)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "spectrogram, filename = iterator.get_next()\n",
    "\n",
    "# all these variables are symbolic until we run them in a session\n",
    "# just like any other tf variable\n",
    "sess = tf.Session()\n",
    "spec, fname = sess.run([spectrogram, filename])\n",
    "\n",
    "sample_rate, waveform = wavfile.read(fname[0])\n",
    "word = str(fname[0]).split(\"/\")[-2].title()\n",
    "print('Word: {}'.format(word))\n",
    "ipd.Audio(data=waveform, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(waveform)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.set_title('\"{}\": Waveform'.format(word))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(spec[0])\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_title('\"{}\": Log Spectrogram'.format(word))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spectrogram looks like an image, use a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Lots of good reasons to do this (local information, accounting for \"high\" and \"low\" voices, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But CNNs are booooooringgggggg..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Baidu <a href=https://arxiv.org/abs/1412.5567>Deep Speech</a>\n",
    "- Recurrent model for end to end speech recognition\n",
    "- Meant for sequence to sequence with different loss function, but forces us to get more creative with `tf.keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=deepspeech.PNG width=\"75%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Need to build custom layers\n",
    "- First rolls consecutive spectrogram frames into single timestep\n",
    "- Second does custom forward and backward recurrence with shared parameters\n",
    "- This is where Keras shines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=deepspeech.PNG width=\"75%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Custom Layers - Concatenate consecutive frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class ImageToDeepSpeech(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_frames, frame_step, **kwargs):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_step = frame_step\n",
    "        super(ImageToDeepSpeech, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.squeeze(inputs, axis=3)\n",
    "        time_slice = lambda x, i: x[:, i:(-(self.num_frames-1)+i) or None:self.frame_step]\n",
    "        time_shifted_inputs = [time_slice(inputs, i) for i in range(self.num_frames)]\n",
    "        return tf.concat(time_shifted_inputs, axis=2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        time_dim = tf.ceil((shape[1] - self.num_frames + 2) / self.frame_step)\n",
    "        feature_dim = self.num_frames*shape[2]\n",
    "        if self.frame_step == 1:\n",
    "            time_dim += 1\n",
    "        return tf.TensorShape([shape[0], time_dim, feature_dim])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(ImageToDeepSpeech, self).get_config()\n",
    "        base_config['num_frames'] = self.num_frames\n",
    "        base_config['frame_step'] = self.frame_step\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Custom Layer - Custom Recurrent Cell\n",
    "Only have to define recurrent step like regular layer. RNN wrapper takes care of rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DeepSpeechCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_size, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        super(DeepSpeechCell, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.matmul(inputs, self.kernel)\n",
    "        u = tf.matmul(prev_output, self.recurrent_kernel)\n",
    "        output = tf.nn.relu(h + u + self.bias)\n",
    "        return output, [output]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[1]\n",
    "        if self.built:\n",
    "            # normally you just return. This is a hack to allow the\n",
    "            # second calling of this cell to use a different recurrent\n",
    "            # kernel. Not elegant but that's showbiz baby\n",
    "            self.recurrent_kernel = self.backward_recurrent_kernel\n",
    "            return\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(self.input_dim, self.state_size),\n",
    "            name='kernel',\n",
    "            initializer='glorot_normal')\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.state_size,),\n",
    "            name='bias',\n",
    "            initializer='zeros')\n",
    "\n",
    "        self.forward_recurrent_kernel = self.add_weight(\n",
    "            shape=(self.state_size, self.state_size),\n",
    "            name='forward_recurrent_kernel',\n",
    "            initializer='glorot_normal')\n",
    "        self.backward_recurrent_kernel = self.add_weight(\n",
    "            shape=(self.state_size, self.state_size),\n",
    "            name='backward_recurrent_kernel',\n",
    "            initializer='glorot_normal')\n",
    "\n",
    "        self.recurrent_kernel = self.forward_recurrent_kernel\n",
    "        super(DeepSpeechCell, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(DeepSpeechCell, self).get_config()\n",
    "        base_config['state_size'] = self.state_size\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def deepspeech_model(\n",
    "        num_frames,\n",
    "        frame_step,\n",
    "        hidden_dims,\n",
    "        num_classes,\n",
    "        dropout=0.05):\n",
    "    # input and convert from image to time series representation\n",
    "    input = tf.keras.Input(shape=(99, 161, 1), name='spec')\n",
    "    x = ImageToDeepSpeech(num_frames, frame_step)(input)\n",
    "\n",
    "    # transform with 3 time distributed dense layers\n",
    "    for n, hdim in enumerate(hidden_dims[:3]):\n",
    "        x = tf.keras.layers.Dropout(dropout, name='dropout_{}'.format(n))(x)\n",
    "        dense = tf.keras.layers.Dense(hdim, activation='relu')\n",
    "        x = tf.keras.layers.TimeDistributed(dense, name='dense_{}'.format(n))(x)\n",
    "\n",
    "    # perform forwards and backwards recurrent layers then combine\n",
    "    # note that we're not returning sequences, so we're going from shape\n",
    "    # B x T x F --> B x F\n",
    "    cell = DeepSpeechCell(hidden_dims[3])\n",
    "    forward = tf.keras.layers.RNN(cell, return_sequences=False, name='forward_rnn')(x)\n",
    "    backward = tf.keras.layers.RNN(cell, return_sequences=False, go_backwards=True, name='backward_rnn')(x)\n",
    "    x = tf.keras.layers.Add(name='rnn_combiner')([forward, backward])\n",
    "\n",
    "    # transform with more dense layers (now not time distributed)\n",
    "    for n, hdim in enumerate(hidden_dims[4:]):\n",
    "        x = tf.keras.layers.Dropout(dropout, name='dropout_{}'.format(n+3))(x)\n",
    "        x = tf.keras.layers.Dense(hdim, activation='relu', name='dense_{}'.format(n+3))(x)\n",
    "\n",
    "    # produce output\n",
    "    x = tf.keras.layers.Dropout(dropout, name='dropout_labels')(x)\n",
    "    x = tf.keras.layers.Dense(num_classes, activation='softmax', name='labels')(x)\n",
    "    return tf.keras.Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 2. A Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "num_cpus = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_input_fn(\n",
    "        dataset_path,\n",
    "        labels,\n",
    "        batch_size,\n",
    "        num_epochs,\n",
    "        mean='/data/mean.npy',\n",
    "        std='/data/std.npy',\n",
    "        eps=0.0001):\n",
    "    def input_fn():\n",
    "        dataset = tf.data.TFRecordDataset([dataset_path])\n",
    "        mean_spec = np.load(mean)\n",
    "        std_spec = np.load(std)**0.5 # saved as variance\n",
    "        table = tf.contrib.lookup.index_table_from_tensor(\n",
    "            mapping=tf.constant(labels),\n",
    "            num_oov_buckets=1)\n",
    "\n",
    "        def parse_spectrogram(record):\n",
    "            features = {\n",
    "                'spec': tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True),\n",
    "                'label': tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
    "            }\n",
    "            parsed = tf.parse_single_example(record, features)\n",
    "\n",
    "            spec = tf.reshape(parsed['spec'], [99, 161]) # Time steps x Frequency bins\n",
    "            spec = (spec - mean_spec) / (std_spec + eps) # normalize\n",
    "            spec = tf.expand_dims(spec, axis=2) # add channel dimension, T x F x 1\n",
    "\n",
    "            label = tf.string_split([parsed['label']], delimiter=\"/\").values[-2:-1]\n",
    "            label = table.lookup(label)[0]\n",
    "            label = tf.one_hot(label, len(labels))\n",
    "            return (spec, label)\n",
    "\n",
    "        # naive approach\n",
    "        dataset = dataset.shuffle(buffer_size=10000)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.map(parse_spectrogram)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        # BREAK IN CASE OF EMERGENCY\n",
    "#         dataset = dataset.apply(\n",
    "#             tf.contrib.data.shuffle_and_repeat(10000, num_epochs))\n",
    "#         dataset = dataset.apply(\n",
    "#             tf.contrib.data.map_and_batch(\n",
    "#                 map_func=parse_spectrogram,\n",
    "#                 batch_size=batch_size,\n",
    "#                 num_parallel_calls=num_cpus))\n",
    "#         dataset.prefetch(buffer_size=None)\n",
    "        return dataset\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 3. A Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!\n",
    "## 3. A Trainer - Estimator API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainer\n",
    "Define some hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Data info\n",
    "TRAIN_DATA = '/data/train.tfrecords'\n",
    "VALID_DATA = '/data/valid.tfrecords'\n",
    "LABELS = '/data/labels.txt'\n",
    "NUM_TRAIN_SAMPLES = 51088\n",
    "\n",
    "# Model hyperparameters\n",
    "NUM_FRAMES = 7\n",
    "FRAME_STEP = 2\n",
    "HIDDEN_SIZES = [1024, 2048, 2048, 1024, 2048]\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 25\n",
    "NUM_GPUS = 1\n",
    "EVAL_THROTTLE_SECS = 120\n",
    "DROPOUT = 0.05\n",
    "\n",
    "# Quick translations\n",
    "STEPS_PER_EPOCH = NUM_TRAIN_SAMPLES // (BATCH_SIZE * NUM_GPUS)\n",
    "MAX_STEPS = NUM_EPOCHS * STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainer\n",
    "Build and compile a tf.keras model using our model function from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS, 'r') as f:\n",
    "    labels = f.read().split(\",\")\n",
    "    labels = labels[:20] + ['unknown']\n",
    "\n",
    "model = deepspeech_model(NUM_FRAMES, FRAME_STEP, HIDDEN_SIZES, len(labels), DROPOUT)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainer\n",
    "Convert our tf.keras model to a TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single GPU config\n",
    "config = tf.estimator.RunConfig(save_checkpoints_steps=STEPS_PER_EPOCH)\n",
    "\n",
    "# BREAK IN CASE OF EMERGENCY\n",
    "# strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS, prefetch_on_device=True)\n",
    "# strategy = tf.contrib.distribute.DistributeConfig(train_distribute=strategy)\n",
    "# config = tf.estimator.RunConfig(\n",
    "#     save_checkpoints_steps=STEPS_PER_EPOCH,\n",
    "#     experimental_distribute=strategy)\n",
    "\n",
    "custom_objects = {\n",
    "    'DeepSpeechCell': DeepSpeechCell,\n",
    "    'ImageToDeepSpeech': ImageToDeepSpeech}\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    model,\n",
    "    custom_objects=custom_objects,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainer\n",
    "Get our data generation functions from the function getter we built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(\n",
    "    TRAIN_DATA,\n",
    "    labels,\n",
    "    BATCH_SIZE,\n",
    "    NUM_EPOCHS)\n",
    "\n",
    "eval_input_fn = get_input_fn(\n",
    "    VALID_DATA,\n",
    "    labels,\n",
    "    BATCH_SIZE*8,\n",
    "    1)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=MAX_STEPS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, throttle_secs=EVAL_THROTTLE_SECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainer\n",
    "Add a metric to keep track of validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    labels = tf.argmax(labels, axis=1)\n",
    "    predictions = tf.argmax(predictions['labels'], axis=1)\n",
    "    return {'accuracy': tf.metrics.accuracy(labels, predictions)}\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainer\n",
    "The moment we've all ben waiting for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keep tabs by running tensorboard --logdir={} --host=0.0.0.0 in a terminal\".format(estimator.model_dir))\n",
    "print(\"Then navigate to <this IP address>:6006/\")\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!\n",
    "## 3. A Trainer - Estimator API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 4. Multi-GPU trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things We Need\n",
    "## 1. A Model - Keras!\n",
    "## 2. A Data Pipeline - Dataset API!\n",
    "## 3. A Trainer - Estimator API!\n",
    "## 4. Multi-GPU trainer -  Distribution API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU\n",
    "Go back to slides above and uncomment the \"Break in case of emergency\" lines, and comment out their existing analogs. Change NUM_GPUs to 4, then click \"Kernel->Restart & Run All\" in the menu bar up top. How does throughput change as we multiply the number of GPUs by 4?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "rise": {
   "autolaunch": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
